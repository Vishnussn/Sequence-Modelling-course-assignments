{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN(Question_3).ipynb","version":"0.3.2","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.12"},"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"}},"cells":[{"metadata":{"id":"l_v4uIUNHX56","colab_type":"code","outputId":"d1f9688c-e0ec-4f78-c059-8b0cb43c74c0","executionInfo":{"status":"ok","timestamp":1556703145337,"user_tz":-330,"elapsed":34565,"user":{"displayName":"R Gowri Prasad","photoUrl":"https://lh6.googleusercontent.com/-cHetS9_ASRE/AAAAAAAAAAI/AAAAAAAAABw/AD16fds3qI4/s64/photo.jpg","userId":"00544449295592809468"}},"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":40}},"cell_type":"code","source":["from google.colab import files\n","uploaded=files.upload()"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-cd62de26-8d00-45c4-82a1-a22b7f0579c7\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-cd62de26-8d00-45c4-82a1-a22b7f0579c7\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "]},"metadata":{"tags":[]}}]},{"metadata":{"id":"cnEnXZ5pkzpY","colab_type":"code","outputId":"bfc4a7a4-bf12-4ae1-a96e-80b905df7789","executionInfo":{"status":"ok","timestamp":1556679779774,"user_tz":-330,"elapsed":29556,"user":{"displayName":"SSN VISHNU","photoUrl":"","userId":"08380309101687309820"}},"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":74}},"cell_type":"code","source":["from google.colab import files\n","uploaded=files.upload()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-cf851f98-8bad-4c4c-b34c-c2e67c2a155e\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-cf851f98-8bad-4c4c-b34c-c2e67c2a155e\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving dev.hi to dev.hi\n"],"name":"stdout"}]},{"metadata":{"id":"a3fsdviATN5y","colab_type":"code","outputId":"1cff60ce-6e93-4d09-9c1b-648b127cbe0e","executionInfo":{"status":"ok","timestamp":1556684651630,"user_tz":-330,"elapsed":794,"user":{"displayName":"SSN VISHNU","photoUrl":"","userId":"08380309101687309820"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["data = open('pg1661.txt', 'r').read()\n","\n","chars = list(set(data)) \n","data_size, vocab_size = len(data), len(chars)\n","print (data_size, vocab_size)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(594933, 93)\n"],"name":"stdout"}]},{"metadata":{"id":"FhFjNmmYaA_V","colab_type":"code","outputId":"0bbcc525-131b-43b0-bad1-eedf6de80838","executionInfo":{"status":"ok","timestamp":1556684693800,"user_tz":-330,"elapsed":2184,"user":{"displayName":"SSN VISHNU","photoUrl":"","userId":"08380309101687309820"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["data1 = open('dev.hi', 'r').read()\n","\n","chars1 = list(set(data1)) \n","data_size1, vocab_size1 = len(data1), len(chars1)\n","print (data_size1, vocab_size1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(123431, 81)\n"],"name":"stdout"}]},{"metadata":{"scrolled":true,"id":"S3p6ckRPHX6F","colab_type":"code","outputId":"eae21f9b-47e4-421f-ff77-db55c962eb94","executionInfo":{"status":"ok","timestamp":1556684796473,"user_tz":-330,"elapsed":854,"user":{"displayName":"SSN VISHNU","photoUrl":"","userId":"08380309101687309820"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"cell_type":"code","source":["char_to_ix = { ch:i for i,ch in enumerate(chars)}\n","ix_to_char={i:ch for i,ch in enumerate(chars)}\n","print char_to_ix\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{' ': 0, '$': 1, '(': 2, ',': 3, '0': 4, '4': 5, '8': 6, '\\xbb': 7, '\\xbf': 8, '@': 9, '\\xc3': 10, 'D': 11, 'H': 12, 'L': 13, 'P': 14, 'T': 15, 'X': 16, 'd': 17, 'h': 18, 'l': 19, '\\xef': 20, 'p': 21, 't': 22, 'x': 23, '\\xa0': 24, '#': 25, \"'\": 26, '\\xa8': 27, '/': 28, '3': 29, '7': 30, ';': 31, '?': 32, 'C': 33, 'G': 34, 'K': 35, 'O': 36, 'S': 37, 'W': 38, '[': 39, 'c': 40, 'g': 41, 'k': 42, 'o': 43, 's': 44, 'w': 45, '\\n': 46, '\"': 47, '&': 48, '\\xa9': 49, '*': 50, '.': 51, '2': 52, '6': 53, ':': 54, 'B': 55, 'F': 56, 'J': 57, 'N': 58, 'R': 59, 'V': 60, 'Z': 61, 'b': 62, 'f': 63, 'j': 64, 'n': 65, 'r': 66, 'v': 67, 'z': 68, '\\r': 69, '!': 70, '\\xa2': 71, '%': 72, ')': 73, '-': 74, '1': 75, '5': 76, '9': 77, 'A': 78, 'E': 79, 'I': 80, 'M': 81, 'Q': 82, 'U': 83, 'Y': 84, ']': 85, 'a': 86, 'e': 87, 'i': 88, 'm': 89, 'q': 90, 'u': 91, 'y': 92}\n"],"name":"stdout"}]},{"metadata":{"id":"3LOn6Eogbc88","colab_type":"code","outputId":"b071820f-c774-47e9-bf8e-593b0c848c1a","executionInfo":{"status":"ok","timestamp":1556684803468,"user_tz":-330,"elapsed":822,"user":{"displayName":"SSN VISHNU","photoUrl":"","userId":"08380309101687309820"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"cell_type":"code","source":["char_to_ix1 = { ch:i for i,ch in enumerate(chars1)}\n","ix_to_char1={i:ch for i,ch in enumerate(chars1)}\n","print char_to_ix1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'\\x81': 0, '\\x80': 1, '\\x83': 2, '\\x82': 3, '\\x85': 4, '\\x87': 6, '\\x86': 7, '\\x89': 8, '\\x88': 9, '\\x8b': 10, '\\n': 11, '\\x8d': 12, '\\x8c': 13, '\\x8f': 14, '\\xac': 44, '\\x91': 16, '\\x90': 17, '\\x93': 18, '\\x95': 19, '\\x94': 20, '\\x97': 21, '\\x96': 22, '5': 80, '\\x98': 24, '\\x9b': 25, '\\x9a': 26, '\\x9d': 27, '\\x9c': 28, '\\x9f': 29, '\\x9e': 30, '\\xa1': 31, ' ': 32, '\\xa3': 33, '\\xa2': 34, '\\xa5': 35, '\\xa4': 36, '\\xa7': 37, '\\xa6': 38, ')': 39, '(': 23, '\\xab': 41, '\\xaa': 42, '\\xad': 43, ',': 15, '\\xaf': 45, '\\xae': 46, '1': 47, '0': 5, '3': 49, '\\xb2': 50, '\\xb5': 51, '4': 52, '\\xb7': 53, '\\xb6': 54, '\\xb9': 55, '\\xb8': 56, ':': 57, '.': 78, '\\xbc': 58, '\\xbf': 59, '\\xbe': 60, '!': 61, '\\xc3': 62, '\\x8a': 63, '7': 64, '\\xa0': 65, '/': 79, '6': 66, '9': 68, '\"': 69, '8': 70, '2': 71, '[': 72, ']': 73, '\\xe0': 74, '-': 67, \"'\": 75, '\\xb0': 48, '\\xa9': 76, '?': 77, '\\xa8': 40}\n"],"name":"stdout"}]},{"metadata":{"id":"EwdNdZkBHX6N","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"d8IJbrIrHX6g","colab_type":"code","colab":{}},"cell_type":"code","source":["#model parameters\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","hiddensize = 100\n","sequencelength = 30\n","learningrate = 1e-1\n","\n","Wxh = np.random.randn(hiddensize, vocab_size) * 0.02 #input to hidden\n","Whh = np.random.randn(hiddensize, hiddensize) * 0.02 #input to hidden\n","Why = np.random.randn(vocab_size, hiddensize) * 0.02 #input to hidden\n","bh = np.zeros((hiddensize, 1))\n","by = np.zeros((vocab_size, 1))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"li5BFVseblAT","colab_type":"code","colab":{}},"cell_type":"code","source":["hiddensize = 100\n","sequencelength = 30\n","learningrate = 1e-1\n","\n","Wxh1 = np.random.randn(hiddensize, vocab_size1) * 0.02 #input to hidden\n","Whh1 = np.random.randn(hiddensize, hiddensize) * 0.02 #input to hidden\n","Why1 = np.random.randn(vocab_size1, hiddensize) * 0.02 #input to hidden\n","bh1 = np.zeros((hiddensize, 1))\n","by1 = np.zeros((vocab_size1, 1))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"U81f1NVVHX6o","colab_type":"text"},"cell_type":"markdown","source":["## Define the loss function\n","\n","The __loss__ is a key concept in all neural networks training. \n","It is a value that describe how good is our model.  \n","The smaller the loss, the better our model is.  \n","(A good model is a model where the predicted output is close to the training output)\n","  \n","During the training phase we want to minimize the loss.\n","\n","The loss function calculates the loss but also the gradients (see backward pass):\n","* It perform a forward pass: calculate the next char given a char from the training set.\n","* It calculate the loss by comparing the predicted char to the target char. (The target char is the input following char in the tranning set)\n","* It calculate the backward pass to calculate the gradients \n","\n","This function take as input:\n","* a list of input char\n","* a list of target char\n","* and the previous hidden state\n","\n","This function outputs:\n","* the loss\n","* the gradient for each parameters between layers\n","* the last hidden state\n"]},{"metadata":{"id":"knGdQkAEHX6q","colab_type":"text"},"cell_type":"markdown","source":["### Forward pass\n","The forward pass use the parameters of the model (Wxh, Whh, Why, bh, by) to calculate the next char given a char from the trainning set.\n","\n","xs[t] is the vector that encode the char at position t\n","ps[t] is the probabilities for next char\n","\n","![alt text](https://deeplearning4j.org/img/recurrent_equation.png \"Logo Title Text 1\")\n","\n","```python\n","hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n","ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n","ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n","```\n","\n","or is dirty pseudo code for each char\n","```python\n","hs = input*Wxh + last_value_of_hidden_state*Whh + bh\n","ys = hs*Why + by\n","ps = normalized(ys)\n","```\n","\n","### Backward pass\n","\n","The naive way to calculate all gradients would be to recalculate a loss for small variations for each parameters.\n","This is possible but would be time consuming.\n","There is a technics to calculates all the gradients for all the parameters at once: the backdrop propagation.  \n","Gradients are calculated in the oposite order of the forward pass, using simple technics.  \n","\n","#### goal is to calculate gradients for the forward formula:\n","```python\n","hs = input*Wxh + last_value_of_hidden_state*Whh + bh  \n","ys = hs*Why + by\n","```\n","\n","The loss for one datapoint\n","![alt text](http://i.imgur.com/LlIMvek.png \"Logo Title Text 1\")\n","\n","How should the computed scores inside f change tto decrease the loss? We'll need to derive a gradient to figure that out.\n","\n","Since all output units contribute to the error of each hidden unit we sum up all the gradients calculated at each time step in the sequence and use it to update the parameters. So our parameter gradients becomes :\n","\n","![alt text](http://i.imgur.com/Ig9WGqP.png \"Logo Title Text 1\")\n","\n","Our first gradient of our loss. We'll backpropagate this via chain rule\n","\n","![alt text](http://i.imgur.com/SOJcNLg.png \"Logo Title Text 1\")\n","\n","The chain rule is a method for finding the derivative of composite functions, or functions that are made by combining one or more functions.\n","\n","![alt text](http://i.imgur.com/3Z2Rfdi.png \"Logo Title Text 1\")\n","\n","![alt text](http://mathpullzone-8231.kxcdn.com/wp-content/uploads/thechainrule-image3.jpg \"Logo Title Text 1\")\n","\n","![alt text](https://i0.wp.com/www.mathbootcamps.com/wp-content/uploads/thechainrule-image1.jpg?w=900 \"Logo Title Text 1\")\n"]},{"metadata":{"id":"6IsW3tmpHX6s","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","def forwardprop(inputs, hprev,vocabsize):\n","  xk, hidden, yk, pk = {}, {}, {}, {} \n","  hidden[-1] = hprev\n","  loss = 0                                                                                                                                                                           \n","  for k in range(0,25):\n","    xk[k] = np.zeros((vocabsize,1))                                                                                                                     \n","    xk[k][inputs[k]] = 1 \n","    hidden[k] = np.tanh(np.dot(Wxh, xk[k]) + np.dot(Whh, hidden[k-1]) + bh)                                                                                                             \n","    yk[k] = np.dot(Why, hidden[k]) + by                                                                                                            \n","    pk[k] = np.exp(yk[k]) / np.sum(np.exp(yk[k]))                                                                                                              \n","    loss += -np.log(pk[k][targets[k],0])                                                                                                    \n","  return pk,hidden,xk,loss\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"Qj0MirW6Kq2R","colab_type":"code","colab":{}},"cell_type":"code","source":["  \n","  def backprop(ps,hs,xs,targets,vocabsize):\n","    dWxh, dWhh, dWhy = np.zeros((100,vocabsize)), np.zeros((100,100)), np.zeros((vocabsize,100))\n","    dbh, dby = np.zeros((100,1)), np.zeros((vocabsize,1))\n","    dhnext = np.zeros_like(hs[0])\n","    for k in range(24,-1,-1):\n","      dy = ps[k]\n","      dy[targets[k]] -= 1 \n","      dWhy += np.dot(dy, np.transpose(hs[k]))\n","      dby += dy\n","      dh = np.dot(np.transpose(Why), dy) + dhnext                                                                                                                                          \n","      dhraw = (1 - hs[k] * hs[k]) * dh                                                                                                                      \n","      dbh += dhraw \n","      dWxh += np.dot(dhraw, np.transpose(xs[k])) \n","      dWhh += np.dot(dhraw, np.transpose(hs[k-1])) \n","      dhnext = np.dot(np.transpose(Whh), dhraw)                                                                                                               \n","    return dWxh, dWhh, dWhy, dbh, dby, hs[24]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"09FW1zdFHX6x","colab_type":"text"},"cell_type":"markdown","source":["## Create a sentence from the model"]},{"metadata":{"id":"NEYeX5piHX6z","colab_type":"code","outputId":"ec5f1190-89e2-4880-b9ed-4378a04e98dc","executionInfo":{"status":"ok","timestamp":1556649340346,"user_tz":-330,"elapsed":917,"user":{"displayName":"SSN VISHNU","photoUrl":"","userId":"08380309101687309820"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"cell_type":"code","source":["\n","def sample(h, seed_ix, n,vocabsize):\n","  x = np.zeros((vocabsize, 1))\n","  x[seed_ix] = 1\n","  indexes = []\n","  for k in xrange(n):\n","    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n","    y = np.dot(Why, h) + by\n","    p = np.exp(y) / np.sum(np.exp(y))\n","    #pick one with the highest probability \n","    index = np.random.choice(range(vocabsize), p=p.ravel())\n","    #create a vector\n","    x = np.zeros((vocabsize, 1))\n","    #customize it for the predicted char\n","    x[index] = 1\n","    #add it to the list\n","    indexes.append(index)\n","\n","  txt = ''.join(ix_to_char[ix] for ix in indexes)\n","  print '----\\n %s \\n----' % (txt, )\n","hprev = np.zeros((hiddensize,1)) # reset RNN memory  \n","#predict the 200 next characters given 'a'\n","sample(hprev,char_to_ix['a'],200)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["----\n"," GJfx#'�Yn%4A#e�kCw]\"' cKS,XUkjVTZZ[\"nQz4l*�;Pz�N4QBNz]L,h!FBAHfIcoWfqfFnn#O4q3,I@f.,wlmVv3x�Mgw@KA6o/2]W�\n","@8-1em?K60pZqv\r ev]$9Xg%�V�I1F:(utQA agOyH1'*�B1h,O$.E[&V,�E-CFXEVf]eJx�3�8n�]y�Jou(Zn )e9�]f? \n","----\n"],"name":"stdout"}]},{"metadata":{"id":"1ITCOY-RprOZ","colab_type":"code","outputId":"113b0e9f-a36f-4142-976d-6017bda940b0","executionInfo":{"status":"error","timestamp":1556685211973,"user_tz":-330,"elapsed":839,"user":{"displayName":"SSN VISHNU","photoUrl":"","userId":"08380309101687309820"}},"colab":{"base_uri":"https://localhost:8080/","height":197}},"cell_type":"code","source":["def sample1(h, seed_ix, n,vocabsize):\n","  x = np.zeros((vocabsize, 1))\n","  x[seed_ix] = 1\n","  indexes = []\n","  for k in xrange(n):\n","    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n","    y = np.dot(Why, h) + by\n","    p = np.exp(y) / np.sum(np.exp(y))\n","    #pick one with the highest probability \n","    index = np.random.choice(range(vocabsize), p=p.ravel())\n","    #create a vector\n","    x = np.zeros((vocabsize, 1))\n","    #customize it for the predicted char\n","    x[index] = 1\n","    #add it to the list\n","    indexes.append(index)\n","\n","  txt = ''.join(ix_to_char1[ix] for ix in indexes)\n","  print '----\\n %s \\n----' % (txt, )\n","hprev = np.zeros((hiddensize,1)) # reset RNN memory  \n","#predict the 200 next characters given 'a'\n","sample1(hprev,char_to_ix1['a'],200)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-23-9a065750c244>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhiddensize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reset RNN memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#predict the 200 next characters given 'a'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0msample1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhprev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchar_to_ix1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyError\u001b[0m: 'a'"]}]},{"metadata":{"id":"bQKCTdg1pghp","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"i5DPYcAPHX65","colab_type":"text"},"cell_type":"markdown","source":["\n","## Training\n","\n","This last part of the code is the main trainning loop:\n","* Feed the network with portion of the file. Size of chunk is *seq_lengh*\n","* Use the loss function to:\n","  * Do forward pass to calculate all parameters for the model for a given input/output pairs\n","  * Do backward pass to calculate all gradiens\n","* Print a sentence from a random seed using the parameters of the network\n","* Update the model using the Adaptative Gradien technique Adagrad"]},{"metadata":{"collapsed":true,"id":"3E83d3a9HX66","colab_type":"text"},"cell_type":"markdown","source":["### Feed the loss function with inputs and targets\n","\n","We create two array of char from the data file,\n","the targets one is shifted compare to the inputs one.\n","\n","For each char in the input array, the target array give the char that follows."]},{"metadata":{"id":"DcMIW9CIHX7A","colab_type":"text"},"cell_type":"markdown","source":["### Adagrad to update the parameters\n","\n","This is a type of gradient descent strategy\n","\n","![alt text](http://www.logos.t.u-tokyo.ac.jp/~hassy/deep_learning/adagrad/adagrad2.png\n"," \"Logo Title Text 1\")\n","\n","\n","\n","step size = learning rate\n","\n","The easiest technics to update the parmeters of the model is this:\n","\n","```python\n","param += dparam * step_size\n","```\n","Adagrad is a more efficient technique where the step_size are getting smaller during the training.\n","\n","It use a memory variable that grow over time:\n","```python\n","mem += dparam * dparam\n","```\n","and use it to calculate the step_size:\n","```python\n","step_size = 1./np.sqrt(mem + 1e-8)\n","```\n","In short:\n","```python\n","mem += dparam * dparam\n","param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update \n","```\n","\n","### Smooth_loss\n","\n","Smooth_loss doesn't play any role in the training.\n","It is just a low pass filtered version of the loss:\n","```python\n","smooth_loss = smooth_loss * 0.999 + loss * 0.001\n","```\n","\n","It is a way to average the loss on over the last iterations to better track the progress\n","\n","\n","### So finally\n","Here the code of the main loop that does both trainning and generating text from times to times:"]},{"metadata":{"id":"wmreIrkLHX7B","colab_type":"code","outputId":"7a7652d3-215d-4105-cd95-4b27f8ee2744","executionInfo":{"status":"error","timestamp":1556685237526,"user_tz":-330,"elapsed":830,"user":{"displayName":"SSN VISHNU","photoUrl":"","userId":"08380309101687309820"}},"colab":{"base_uri":"https://localhost:8080/","height":231}},"cell_type":"code","source":["n, p = 0, 0\n","mWxh, mWhh, mWhy = np.zeros((100,93)), np.zeros((100,100)), np.zeros((93,100))\n","mbh, mby = np.zeros((100,1)), np.zeros((93,1))\n","error=[]\n","epoch=[]\n","for n in range (2000):\n","  if p+sequencelength+1 >= len(data) or n == 0:\n","    hprev = np.zeros((hiddensize,1))                                                                                                                                      \n","    p = 0                                                                                                                                                              \n","  inputs = [char_to_ix[ch] for ch in data[p:p+sequencelength]]\n","  targets = [char_to_ix[ch] for ch in data[p+1:p+sequencelength+1]]\n"," \n","  ps,hs,xs,loss=forwardprop(inputs,hprev,93)\n","  dWxh, dWhh, dWhy, dbh, dby, hs=backprop(ps,hs,xs,targets)\n","  if n%20==0:                                                                                                                                                     \n","    #print 'iter %d, loss: %f' % (n, loss) \n","    error.append(loss)\n","    epoch.append(n)\n","    sample(hprev, inputs[0], 200)\n","  mWxh+=dWxh*dWxh\n","  mWhh+=dWhh*dWhh\n","  mWhy+=dWhy*dWhy\n","  mbh+=dbh*dbh\n","  mby+=dby*dby\n","  Wxh=Wxh-learningrate*dWxh/np.sqrt(mWxh+1e-5)\n","  Whh=Whh-learningrate*dWhh/np.sqrt(mWhh+1e-5)\n","  Why=Why-learningrate*dWhy/np.sqrt(mWhy+1e-5)\n","  bh=bh-learningrate*dbh/np.sqrt(mbh+1e-5)\n","  by=by-learningrate*dby/np.sqrt(mby+1e-5)\n","  p += sequencelength                                                                                                                                                         \n","  n += 1     \n","plt.plot(error)\n","    "],"execution_count":0,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-24-469bac30b795>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_to_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msequencelength\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforwardprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: forwardprop() takes exactly 3 arguments (2 given)"]}]},{"metadata":{"id":"PsJocOfhdT7K","colab_type":"code","colab":{}},"cell_type":"code","source":["n, p = 0, 0\n","mWxh, mWhh, mWhy = np.zeros((100,81)), np.zeros((100,100)), np.zeros((81,100))\n","mbh, mby = np.zeros((100,1)), np.zeros((93,1))\n","error=[]\n","epoch=[]\n","for n in range (2000):\n","  if p+sequencelength+1 >= len(data) or n == 0:\n","    hprev = np.zeros((hiddensize,1))                                                                                                                                      \n","    p = 0                                                                                                                                                              \n","  inputs = [char_to_ix1[ch] for ch in data1[p:p+sequencelength]]\n","  targets = [char_to_ix1[ch] for ch in data1[p+1:p+sequencelength+1]]\n"," \n","  ps,hs,xs,loss=forwardprop(inputs,hprev)\n","  dWxh, dWhh, dWhy, dbh, dby, hs=backprop(ps,hs,xs,targets)\n","  if n%20==0:                                                                                                                                                     \n","    #print 'iter %d, loss: %f' % (n, loss) \n","    error.append(loss)\n","    epoch.append(n)\n","    sample(hprev, inputs[0], 200)\n","  mWxh+=dWxh*dWxh\n","  mWhh+=dWhh*dWhh\n","  mWhy+=dWhy*dWhy\n","  mbh+=dbh*dbh\n","  mby+=dby*dby\n","  Wxh=Wxh-learningrate*dWxh/np.sqrt(mWxh+1e-5)\n","  Whh=Whh-learningrate*dWhh/np.sqrt(mWhh+1e-5)\n","  Why=Why-learningrate*dWhy/np.sqrt(mWhy+1e-5)\n","  bh=bh-learningrate*dbh/np.sqrt(mbh+1e-5)\n","  by=by-learningrate*dby/np.sqrt(mby+1e-5)\n","  p += sequencelength                                                                                                                                                         \n","  n += 1     \n","plt.plot(error)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Flg2pw3kHX7H","colab_type":"text"},"cell_type":"markdown","source":[""]}]}